# -*- coding: utf-8 -*-
"""mediapipe_hands.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FvH5eTiZqayZBOHZsFm-i7D-JvoB9DVz

Usage example of MediaPipe Hands Solution API in Python (see also http://solutions.mediapipe.dev/hands).
"""

#!pip install numpy==1.19.3
#!pip install mediapipe

"""Upload any image that contains hand(s) to the Colab. We took two examples from the web: https://unsplash.com/photos/QyCH5jwrD_A and https://unsplash.com/photos/tSePVHkxUCk

"""
import os
import numpy as np
import cv2
import mediapipe as mp
import matplotlib.pyplot as plt
import math
import statistics
import argparse
from scipy import signal
from scipy.signal import argrelextrema
from scipy.fftpack import fft,ifft
from scipy.signal import butter, lfilter
from scipy.signal import freqs
import gi
gi.require_version('Gtk', '3.0')
from gi.repository import Gtk


smooth = 0.5
thres = 0.2


total_count=0
amp_total =0

time=[]
cnt=0
dis= []
inv_dis =[]
inv_inv_dis =[]
dis_right = []
inv_dis_r =  []
inv_inv_dis_r =[]
dis_left =[]
inv_dis_l =[]
inv_inv_dis_l = []
case = 0 # 0:Single hand 1:Right hand 2:Left hand
mid=0
record= []




def reject_outliers(arr):
    dis = np.array(arr)
    u = np.mean(dis,axis=0)
    s = np.std(dis,axis=0)
    final_list = [x for x in arr if (x > u - 2*s)]
    final_list = [x for x in final_list if (x < u + 2*s)]
    return final_list



# Function finding peak position
def peak(dis,Threshold):

    # Use scipy.find_peaks to find peak
    peaks = signal.find_peaks(dis,height=Threshold,distance=6)
    peaks_pos = peaks[0]

    ### reject_peak_outliners
    dis_peak = []
    for i in range(len(peaks_pos)):
        dis_peak.append(dis[peaks_pos[i]])

    new_dis_peak = reject_outliers(dis_peak)

    new_peak_pos= []
    for i in range(len(peaks_pos)):
        for j in range(len(new_dis_peak)):
            if dis[peaks_pos[i]] == new_dis_peak[j]:
                new_peak_pos.append(peaks_pos[i])

    for i in range(len(new_peak_pos)):
        plt.plot(new_peak_pos[i],dis[new_peak_pos[i]],marker='o',color='blue')


    ##Write output
    print("Peak Frame:",new_peak_pos)
    file.write("Peak Frame: [ ")
    file.writelines(["%s " % i  for i in new_peak_pos])
    file.write("]\n")

    return new_peak_pos



# Function finding valley position and tapping number
def valley(inv_dis,Threshold,widths,count):

    # Use scipy.find_peaks to find valley
    valleys = signal.find_peaks(inv_dis,height=-Threshold,distance=6)
    valleys_pos= valleys[0]


    ##Write output
    print("Tapping Frame:",valleys_pos)
    file.write("Tapping Frame: [ ")
    file.writelines(["%s " % i  for i in valleys_pos])
    file.write("]\n")

    valley_indexes = signal.find_peaks_cwt(inv_dis, widths)

    for i in range(len(inv_dis)):
        inv_inv_dis.append(inv_dis[i]*(-1))


    for i in range(len(valleys_pos)):
        plt.plot(valleys_pos[i],inv_inv_dis[valleys_pos[i]],marker='o',color='red')
        count +=1

    return valleys_pos, count

# Parser

parser = argparse.ArgumentParser()
parser.add_argument("-p","--path", type=str,default='.', help='input video path')
parser.add_argument("-i","--input_name", type=str, help='input video name')
parser.add_argument("--hand", type=str, default='right', choices=['right', 'left'], help='output video name')

args = parser.parse_args()


### read arg
path = args.path
input_name= args.input_name
parts = args.input_name.split('.')
output_name = parts[0]+"_result"
input= path+"/"+input_name
output = path +"/" + output_name +".mp4"


### open file to write the result in
file= open(output_name+".txt","w")


# Read video with OpenCV.
cap=cv2.VideoCapture(input)

## Get video info
RES=(round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))
fps = round(cap.get(cv2.CAP_PROP_FPS))
out_fps = fps*2


fourcc = cv2.VideoWriter_fourcc(*'MP4V')

out = cv2.VideoWriter(output, fourcc, out_fps, RES)

while(cap.isOpened()):
    ret, frame = cap.read()


    if ret == True:
        mp_hands = mp.solutions.hands

        # Initialize MediaPipe Hands.
        hands = mp_hands.Hands(
            static_image_mode=True,
            max_num_hands=2,
            min_detection_confidence=0.7)

        mp_drawing = mp.solutions.drawing_utils

        # Convert the BGR image to RGB, flip the image around y-axis for correct
        # handedness output and process it with MediaPipe Hands.

        results = hands.process(cv2.flip(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 1))
        image_hight, image_width, _ = frame.shape
        #print(results.multi_handedness)



        # Draw hand landmarks of each hand.
        # if not results.multi_hand_landmarks:
        #   #out.write(frame)
        #   # cv2.imshow('frame',frame)
        #   # cv2.waitKey(0)
        #
        #   continue
        annotated_image = cv2.flip(frame.copy(), 1)

        if results.multi_hand_landmarks != None:

            for hand_landmarks in results.multi_hand_landmarks:
                    Wrist_x = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x * image_width
                    record.append(Wrist_x)


            for i in range(len(record)):
                mid+=record[i]

            mid=mid/len(record)



            for hand_landmarks in results.multi_hand_landmarks:
              # Print index finger tip coordinates.



                Thumb_x = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x * image_width
                Thumb_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y * image_hight

                Thumb_t_x= hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC].x * image_width
                Thumb_t_y= hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC].y * image_width


                Index_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width
                Index_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight

                Index_t_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].x * image_width
                Index_t_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y * image_width

                Wrist_x = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x * image_width
                Wrist_y = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y * image_width



                ThumbTipToIndexTip = math.sqrt(np.square((Thumb_x - Index_x)) + np.square((Thumb_y - Index_y)))
                ThumbTailToIndexTail = math.sqrt(np.square((Thumb_t_x - Index_t_x)) + np.square((Thumb_t_y - Index_t_y)))


            # Multi-hand right or left hand tapping (Note the person must face to the camera to find left right hand)

                if len(results.multi_handedness) > 1:
                    if (args.hand =="right"):
                        # right hand tapping
                        case=1
                        if(Wrist_x>mid):
                            dis_right.append(ThumbTipToIndexTip/ThumbTailToIndexTail)
                    else:
                        # left hand tapping
                        case=2
                        if(Wrist_x<mid):
                            dis_left.append(ThumbTipToIndexTip/ThumbTailToIndexTail)
                else:
                    #print("Single-hand")
                    dis.append(ThumbTipToIndexTip/ThumbTailToIndexTail)

                mp_drawing.draw_landmarks(
                    annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                out.write(cv2.flip(annotated_image, 1))

                time.append(cnt)
                cnt  += 1

    else:
        break


cap.release()
out.release()
cv2.destroyAllWindows()


### Smoothing with parameter smooth and Find Threshold with thres

if(case==0):
    y = sorted(dis)
    Threshold = y[math.floor(cnt*thres)]
    for i in range(len(dis)):
        dis[i]= dis[i-1]*smooth + dis[i]*(1-smooth)
else:
    if (case==1):
        sort_dis_right = sorted(dis_right)
        Threshold = sort_dis_right[math.floor(cnt*thres)]
        for i in range(len(dis_right)-1):
            dis_right[i]= dis_right[i-1]*smooth + dis_right[i]*(1-smooth)

    else:
        sort_dis_left = sorted(dis_left)
        Threshold = sort_dis_left[math.floor(cnt*thres)]
        for i in range(len(dis_left)-1):
            dis_left[i]= dis_left[i-1]*smooth + dis_left[i]*(1-smooth)


print('Threshold:',Threshold)




# print('Case:',case)



#Single Hand
if(case==0):
    plt.plot(time,dis)
    widths= np.arange(1,cnt-1)
    for i in range(len(dis)):
        inv_dis.append(dis[i]*(-1))


    # Find Peak
    peaks_pos= peak(dis,Threshold)

    # Find Valley and count tap numbers
    valleys_pos,total_count = valley(inv_dis,Threshold,widths,total_count)


    # cal amp
    # for i in range(len(peaks_pos)):
    #     amp_total += (dis[peaks_pos[i]]-dis[valleys_pos[i]])
    # amp = amp_total/len(peaks_pos)
    # print('Amplitude :',amp)

# Muti-hands Right hand tapping # Muti-hands Left hand tapping
else:
    #right
    if (case==1):
        plt.plot(time[0:len(dis_right)],dis_right)
        widths= np.arange(1,cnt/2-1)
        for i in range(len(dis_right)):
            inv_dis_r.append(dis_right[i]*(-1))

        # Find Peak
        peaks_pos= peak(dis_right,Threshold)

        # Find Valley and count tap numbers
        valleys_pos,total_count = valley(inv_dis_r,Threshold,widths,total_count)

        #cal amp

        # for i in range(len(peaks_pos)):
        #     amp_total += (dis_right[peaks_pos[i]]-dis_right[valleys_pos[i]])
        # amp_right= amp_total/len(peaks_pos)
        # print('Amplitude :',amp_right)
    # Left
    else:
        plt.plot(time[0:len(dis_left)],dis_left)
        widths= np.arange(1,cnt/2-1)

        for i in range(len(dis_left)):
            inv_dis_l.append(dis_left[i]*(-1))


        # Find Peak
        peaks_pos= peak(dis_left,Threshold)

        # Find Valley and count tap numbers
        valleys_pos,total_count = valley(inv_dis_l,Threshold,widths,total_count)

        #cal amp

        # for i in range(len(peaks_pos)):
        #     amp_total += (dis_left[peaks_pos[i]]-dis_left[valleys_pos[i]])
        # amp_left= amp_total/len(peaks_pos)
        # print('Amplitude :',amp_left)


### caculate the action time, Regularity and tapping Frequency
action_time = (max(peaks_pos[len(peaks_pos)-1], valleys_pos[len(valleys_pos)-1]) - min(peaks_pos[0],valleys_pos[0]))/ fps


reg=[]
for i in range(len(valleys_pos)):
    if(i!=0):
        reg.append(valleys_pos[i]-valleys_pos[i-1])

tapping_freq = total_count/action_time


### print out the final result 
print("Tap number:",total_count, "times")
print("Action duration:",action_time, "seconds")
print("Tapping Frequency",tapping_freq)
print("Regularity:",np.var(reg))

file.write("Tap number: " + str(total_count) + " times\n")
file.write("Action duration: " + str(action_time) + " seconds\n")
file.write("Tapping Frequency:" + str(tapping_freq)+ "\n")
file.write("Regularity: " + str(np.var(reg))+"\n" )

file.close()


#results.multi_handedness[0].classification[0].label



plt.show()
plt.savefig(output_name+".png")
    #cv2.imshow('Image',annotated_image)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()
    #cv2.imwrite(output_path+"/"+filename,annotated_image)


# Preview the images.

"""All MediaPipe Solutions Python API examples are under mp.solutions.

For the MediaPipe Hands solution, we can access this module as `mp_hands = mp.solutions.hands`.

You may change the parameters, such as `static_image_mode`, `max_num_hands`, and `min_detection_confidence`, during the initialization. Run `help(mp_hands.Hands)` to get more informations about the parameters.
"""
